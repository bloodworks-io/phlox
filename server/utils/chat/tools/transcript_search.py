"""
Transcript search tool implementation.

This tool searches through the patient transcript for relevant information.
"""

import logging
from typing import Any, AsyncGenerator, Dict

from server.utils.chat.streaming.response import (
    status_message,
    stream_llm_response,
    tool_response_message,
)
from server.utils.helpers import clean_think_tags

logger = logging.getLogger(__name__)


async def execute(
    tool_call: Dict[str, Any],
    llm_client,
    config: Dict[str, Any],
    message_list: list,
    conversation_history: list,
    raw_transcription: str,
    context_question_options: Dict[str, Any],
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    Execute the transcript search tool.

    Args:
        tool_call: The tool call to execute.
        llm_client: The LLM client instance.
        config: The configuration dictionary.
        message_list: The current message list.
        conversation_history: The conversation history.
        raw_transcription: The raw transcript text.
        context_question_options: The context question options.

    Yields:
        Dict[str, Any]: Streaming response chunks.
    """
    logger.info("Executing transcript_search tool...")

    # Check if transcript is available
    if not raw_transcription:
        logger.info("No transcript available.")
        yield status_message("Generating response...")

        message_list.append(
            tool_response_message(
                tool_call_id=tool_call.get("id", ""),
                content="No transcript is available to query. Please answer the user's question without transcript information.",
            )
        )

        async for chunk in stream_llm_response(
            llm_client=llm_client,
            model=config["PRIMARY_MODEL"],
            messages=message_list,
            options=context_question_options,
        ):
            yield chunk
    else:
        logger.info("Searching transcript for query...")
        yield status_message("Searching through transcript...")

        # Create a query to extract information from the transcript
        query = conversation_history[-1]["content"]

        # Create a new message list with the transcript and query
        transcript_query_messages = [
            {
                "role": "system",
                "content": "You are a helpful medical assistant. Extract the relevant information from the provided transcript to answer the user's question. Only include information that is present in the transcript and include direct quotes. The transcript was generated by an automated system therefore it may contain errors.",
            },
            {
                "role": "user",
                "content": f"Here is the transcript of a patient conversation:\n\n{raw_transcription}\n\nBased on this transcript only, please answer the following question: {query}",
            },
        ]

        # Get information from transcript
        transcript_response = await llm_client.chat(
            model=config["PRIMARY_MODEL"],
            messages=transcript_query_messages,
            options=context_question_options,
        )

        transcript_info = transcript_response.get("message", {}).get(
            "content", ""
        )

        # Clean think tags
        cleaned_transcript_info = ""
        cleaned_result = clean_think_tags([{"content": transcript_info}])
        if (
            cleaned_result
            and len(cleaned_result) > 0
            and isinstance(cleaned_result[0], dict)
            and "content" in cleaned_result[0]
        ):
            cleaned_transcript_info = str(cleaned_result[0].get("content", ""))

        logger.info(
            f"Transcript query result: {cleaned_transcript_info[:200]}..."
        )

        # Add transcript info to original conversation as a tool response
        message_list.append(
            tool_response_message(
                tool_call_id=tool_call.get("id", ""),
                content=f"The following information was found in the transcript:\n\n{cleaned_transcript_info}",
            )
        )

        # Clean think tags again
        cleaned_message_list = clean_think_tags(message_list)

        logger.info(f"Transcript query messagelist: {cleaned_message_list}")
        yield status_message(
            "Generating response with transcript information..."
        )

        logger.info("Starting response stream to frontend")

        # Stream the answer
        async for chunk in stream_llm_response(
            llm_client=llm_client,
            model=config["PRIMARY_MODEL"],
            messages=cleaned_message_list,
            options=context_question_options,
        ):
            yield chunk
